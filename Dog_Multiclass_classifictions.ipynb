{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joovence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8cX9PHZHtAF"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os \n",
        "import cv2\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "901MHe6MIYMj"
      },
      "source": [
        "# data: https://drive.google.com/file/d/1kK6lDJQ6KRQ5Gg0sX4Kh5arbf5gIRAB1\n",
        "!tar -xf /content/drive/MyDrive/dog_breeds.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKdQ9nQImAJ"
      },
      "source": [
        "def load_pictures(path):\n",
        "  X = []; i = 0; y = []\n",
        "  for folder in os.listdir(path):\n",
        "    for picture in os.listdir(path+folder):\n",
        "      pic = cv2.imread(os.path.join(path+folder,picture))\n",
        "      try:\n",
        "        pic = cv2.resize(pic,(int(128),int(128)))\n",
        "        pic = cv2.cvtColor(pic, cv2.COLOR_BGR2GRAY)\n",
        "        pic = (pic - 127.5) / 127.5\n",
        "        X.append(pic.astype('float32'))\n",
        "        y.append(i)\n",
        "      except:\n",
        "        continue\n",
        "    i += 1\n",
        "    print(folder)\n",
        "  return [np.asarray(X), np.asarray(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qshrhsc6AJ_Y"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Softmax, Dropout, Conv2D, MaxPooling2D, ReLU, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import losses, Sequential\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.data import Dataset\n",
        "from imgaug import augmenters as aug\n",
        "from tensorflow.keras.initializers import RandomNormal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xioh-FIBF6Sc"
      },
      "source": [
        "def build_pseudo_label(img_shape):\n",
        "       \n",
        "    ALPHA = 0.1\n",
        "    \n",
        "    # Initialisateur qui génère des tenseurs avec une distribution normale\n",
        "    init = RandomNormal(mean=0.0, stddev=0.02) \n",
        "    \n",
        "    # Espace latent\n",
        "    input = Input(shape=(img_shape))\n",
        "    \n",
        "    # 7x7\n",
        "    X = Conv2D(128, kernel_size=  (7,7) , kernel_initializer = init)(input)\n",
        "    X = LeakyReLU(ALPHA)(X)\n",
        "    \n",
        "    # 14x14\n",
        "    X = Conv2D(128, kernel_size = (4,4), strides=(2,2), padding='same', kernel_initializer = init)(X)\n",
        "    X = LeakyReLU(ALPHA)(X)\n",
        "    \n",
        "    # 28x28\n",
        "    X = Conv2D(128, kernel_size = (4,4), strides=(2,2), padding='same', kernel_initializer = init)(X)\n",
        "    X = LeakyReLU(ALPHA)(X)\n",
        "    \n",
        "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "    X = Dropout(0.25)(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(128)(X)\n",
        "    X = ReLU()(X)\n",
        "    X = Dropout(0.4)(X)\n",
        "    out_layer = Dense(4, activation='softmax')(X)\n",
        "    model = Model(input, out_layer)\n",
        "    \n",
        "    # Définir le modèle\n",
        "    model = Model(input, out_layer)\n",
        "    \n",
        "    '''\n",
        "    input = Input(shape=img_shape)\n",
        "    X = Conv2D(256, kernel_size=(5, 5))(input)\n",
        "    X = ReLU()(X)\n",
        "    X = Conv2D(128, kernel_size=(3, 3))(X)\n",
        "    X = ReLU()(X)\n",
        "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "    X = Dropout(0.25)(X)\n",
        "    X = Conv2D(128, kernel_size=(3, 3))(input)\n",
        "    X = ReLU()(X)\n",
        "    X = Conv2D(64, kernel_size=(3, 3))(X)\n",
        "    X = ReLU()(X)\n",
        "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(256)(X)\n",
        "    X = ReLU()(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X = Dense(128)(X)\n",
        "    X = ReLU()(X)\n",
        "    out_layer = Dense(4, activation='softmax')(X)\n",
        "    model = Model(input, out_layer)\n",
        "    '''\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7_kj8LMF3OH"
      },
      "source": [
        "def select_samples(data, source, target, nbr_classes=4):\n",
        "    X, y = data\n",
        "    X_list, y_list = list(), list()\n",
        "    for i in range(nbr_classes):\n",
        "        X_with_class = X[y == i]        \n",
        "        ix = range(source, target)\n",
        "        [X_list.append(X_with_class[j]) for j in ix]\n",
        "        [y_list.append(i) for j in ix]\n",
        "    return np.asarray(X_list), np.asarray(y_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9aDEBir2lJ9"
      },
      "source": [
        "def augmenter(augmenter):\n",
        "\n",
        "        if augmenter == 'weak':\n",
        "                      augmenter = aug.Sequential([\n",
        "                      aug.Sometimes(1.0, aug.GaussianBlur((0.5, 0.6))),\n",
        "                      aug.Sometimes(0.8, aug.Affine(rotate=(-5, 5)))],\n",
        "                      random_order=True)\n",
        "        elif augmenter == 'strong':\n",
        "            augmenter = aug.Sequential([\n",
        "                      aug.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                      aug.Sometimes(0.8, aug.Affine(rotate=(-25, 25))),\n",
        "                      aug.Sometimes(0.5, aug.Affine(\n",
        "                      scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "                      translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                      rotate=(-25, 25),\n",
        "                      shear=(12, 15)))],\n",
        "                      random_order=True)\n",
        "        else:\n",
        "            raise KeyError('Unknown augmentation {!r}'.format(augmenter))\n",
        "        \n",
        "        return augmenter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpfKgww2vfm",
        "outputId": "d92ed485-4da3-4495-8806-7a35a0760b8e"
      },
      "source": [
        "BATCH_SIZE = 120\n",
        "BUFFER_SIZE = 1000\n",
        "INPUT_SHAPE = (128, 128, 1)\n",
        "\n",
        "data = load_pictures('/content/train/')\n",
        "\n",
        "X_train, y_train = select_samples(data, 0, 30, 4)\n",
        "#X_train_unlabelled, y_train_unlabelled = select_samples(data, 10, 40, 4)\n",
        "X_eval, y_eval = select_samples(data, 30, 49, 4)\n",
        "\n",
        "train_data = Dataset.from_tensor_slices((X_train,y_train))\n",
        "train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "weak_augmenter = augmenter(\"weak\")\n",
        "strong_augmenter = augmenter(\"strong\")\n",
        "\n",
        "x_train_weak = []\n",
        "x_train_strong = []\n",
        "y_train_weak = []\n",
        "y_train_strong = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  for j in range(30):\n",
        "    x_train_weak.append(weak_augmenter.augment_image(X_train[i]))\n",
        "    y_train_weak.append(y_train[i])\n",
        "    x_train_strong.append(strong_augmenter.augment_image(X_train[i]))\n",
        "    y_train_strong.append(y_train[i])\n",
        "    \n",
        "X_train_aug = x_train_weak + x_train_strong\n",
        "y_train_aug = y_train_weak + y_train_strong\n",
        "X_train_aug = np.array(X_train_aug)\n",
        "y_train_aug = np.array(y_train_aug)\n",
        "\n",
        "#pseudo_data = Dataset.from_tensor_slices((X_train_unlabelled))\n",
        "#pseudo_data = pseudo_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "eval_data = Dataset.from_tensor_slices((X_eval,y_eval))\n",
        "eval_data = eval_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bulldog\n",
            "dalmatian\n",
            "husky\n",
            "labrador\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTHw3o8FVqmZ"
      },
      "source": [
        "X_train = tf.keras.layers.Concatenate(axis=0)([X_train, X_train_aug])\n",
        "y_train = np.concatenate([y_train, y_train_aug])\n",
        "\n",
        "train_data = Dataset.from_tensor_slices((X_train,y_train))\n",
        "train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gB8fvoB12yTi",
        "outputId": "e27d0c46-b578-4a0b-d7e3-b46c6b8f676a"
      },
      "source": [
        "pseudo_label_classifier = build_pseudo_label(INPUT_SHAPE)\n",
        "pseudo_label_classifier.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "pseudo_label_classifier.fit(train_data, validation_data=eval_data, batch_size=BATCH_SIZE, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 1.2286 - accuracy: 0.4394 - val_loss: 1.6096 - val_accuracy: 0.4605\n",
            "Epoch 2/100\n",
            "61/61 [==============================] - 24s 386ms/step - loss: 1.0486 - accuracy: 0.6270 - val_loss: 1.7904 - val_accuracy: 0.5658\n",
            "Epoch 3/100\n",
            "61/61 [==============================] - 23s 371ms/step - loss: 0.6925 - accuracy: 0.7435 - val_loss: 1.8306 - val_accuracy: 0.5658\n",
            "Epoch 4/100\n",
            "61/61 [==============================] - 22s 369ms/step - loss: 0.3362 - accuracy: 0.8705 - val_loss: 1.7757 - val_accuracy: 0.5526\n",
            "Epoch 5/100\n",
            "61/61 [==============================] - 23s 376ms/step - loss: 0.2740 - accuracy: 0.8917 - val_loss: 1.6374 - val_accuracy: 0.5789\n",
            "Epoch 6/100\n",
            "61/61 [==============================] - 23s 376ms/step - loss: 0.2026 - accuracy: 0.9263 - val_loss: 1.7360 - val_accuracy: 0.6184\n",
            "Epoch 7/100\n",
            "61/61 [==============================] - 23s 373ms/step - loss: 0.1150 - accuracy: 0.9603 - val_loss: 1.5768 - val_accuracy: 0.5789\n",
            "Epoch 8/100\n",
            "61/61 [==============================] - 23s 373ms/step - loss: 0.1290 - accuracy: 0.9558 - val_loss: 1.4760 - val_accuracy: 0.5526\n",
            "Epoch 9/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0928 - accuracy: 0.9678 - val_loss: 2.0091 - val_accuracy: 0.5263\n",
            "Epoch 10/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0841 - accuracy: 0.9686 - val_loss: 2.4632 - val_accuracy: 0.5789\n",
            "Epoch 11/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0745 - accuracy: 0.9707 - val_loss: 2.0466 - val_accuracy: 0.6184\n",
            "Epoch 12/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0740 - accuracy: 0.9737 - val_loss: 2.0317 - val_accuracy: 0.5921\n",
            "Epoch 13/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0457 - accuracy: 0.9827 - val_loss: 2.2184 - val_accuracy: 0.5658\n",
            "Epoch 14/100\n",
            "61/61 [==============================] - 23s 376ms/step - loss: 0.0426 - accuracy: 0.9823 - val_loss: 2.3984 - val_accuracy: 0.6842\n",
            "Epoch 15/100\n",
            "61/61 [==============================] - 23s 373ms/step - loss: 0.0389 - accuracy: 0.9861 - val_loss: 2.1581 - val_accuracy: 0.6579\n",
            "Epoch 16/100\n",
            "61/61 [==============================] - 23s 373ms/step - loss: 0.0313 - accuracy: 0.9882 - val_loss: 2.3752 - val_accuracy: 0.6579\n",
            "Epoch 17/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0316 - accuracy: 0.9885 - val_loss: 2.4210 - val_accuracy: 0.6316\n",
            "Epoch 18/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0336 - accuracy: 0.9873 - val_loss: 2.7282 - val_accuracy: 0.6711\n",
            "Epoch 19/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0236 - accuracy: 0.9911 - val_loss: 2.4296 - val_accuracy: 0.6447\n",
            "Epoch 20/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0184 - accuracy: 0.9940 - val_loss: 2.5837 - val_accuracy: 0.6579\n",
            "Epoch 21/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 2.6386 - val_accuracy: 0.6184\n",
            "Epoch 22/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 2.8781 - val_accuracy: 0.6447\n",
            "Epoch 23/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 2.8532 - val_accuracy: 0.6447\n",
            "Epoch 24/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0169 - accuracy: 0.9938 - val_loss: 3.0823 - val_accuracy: 0.5526\n",
            "Epoch 25/100\n",
            "61/61 [==============================] - 23s 375ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 3.0511 - val_accuracy: 0.6053\n",
            "Epoch 26/100\n",
            "61/61 [==============================] - 23s 374ms/step - loss: 0.0217 - accuracy: 0.9938 - val_loss: 2.8051 - val_accuracy: 0.6053\n",
            "Epoch 27/100\n",
            "27/61 [============>.................] - ETA: 12s - loss: 0.0031 - accuracy: 0.9990"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-85a02154a032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpseudo_label_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aet9x90r7jC"
      },
      "source": [
        "pseudo_label_classifier.save('/content/drive/MyDrive/classifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJxtEzb0rkRF"
      },
      "source": [
        "# Ladder Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niB2rLKYr1Xh"
      },
      "source": [
        "# Chargement des bibliothèques\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import keras\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgM0FJDirwHi"
      },
      "source": [
        "X = data[0]\n",
        "y = data[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)\n",
        "\n",
        "dim = X_train.shape[1]*X_train.shape[2]\n",
        "# Mettre les images en format matrice avec la fonction reshape()\n",
        "X_train = X_train.reshape(159, dim)\n",
        "\n",
        "dim = X_test.shape[1]*X_test.shape[2]\n",
        "# Mettre les images en format matrice avec la fonction reshape()\n",
        "X_test = X_test.reshape(40, dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJME9earnbA"
      },
      "source": [
        "class AddBeta(Layer):\n",
        "    def __init__(self  , **kwargs):\n",
        "        super(AddBeta, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.built:\n",
        "            return\n",
        "        \n",
        "        self.beta = self.add_weight(name='beta', \n",
        "                                      shape= input_shape[1:] ,\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "        self.built = True\n",
        "        super(AddBeta, self).build(input_shape)  \n",
        "        \n",
        "    def call(self, x, training=None):\n",
        "        return tf.add(x, self.beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_KxkUhjsbmn"
      },
      "source": [
        "# Méthode de débruitage proposée dans l'article\n",
        "class G_Guass(Layer):\n",
        "    def __init__(self , **kwargs):\n",
        "        super(G_Guass, self).__init__(**kwargs)\n",
        "        \n",
        "    def wi(self, init, name):\n",
        "        if init == 1:\n",
        "            return self.add_weight(name='guess_'+name, \n",
        "                                      shape=(self.size,),\n",
        "                                      initializer='ones',\n",
        "                                      trainable=True)\n",
        "        elif init == 0:\n",
        "            return self.add_weight(name='guess_'+name, \n",
        "                                      shape=(self.size,),\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid argument '%d' provided for init in G_Gauss layer\" % init)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Créer une variable de poids (entraînable) pour cette couche\n",
        "        self.size = input_shape[0][-1]\n",
        "\n",
        "        init_values = [0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]\n",
        "        self.a = [self.wi(v, 'a' + str(i + 1)) for i, v in enumerate(init_values)]\n",
        "        super(G_Guass , self).build(input_shape)  # N'oubliez pas de l'appeler à la fin\n",
        "\n",
        "    def call(self, x):\n",
        "        z_c, u = x \n",
        "\n",
        "        def compute(y):\n",
        "            return y[0] * tf.sigmoid(y[1] * u + y[2]) + y[3] * u + y[4]\n",
        "\n",
        "        mu = compute(self.a[:5])\n",
        "        v  = compute(self.a[5:])\n",
        "\n",
        "        z_est = (z_c - mu) * v + mu\n",
        "        return z_est\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], self.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUv-Qao7sfis"
      },
      "source": [
        "def batch_normalization(batch, mean=None, var=None):\n",
        "    if mean is None or var is None:\n",
        "        mean, var = tf.nn.moments(batch, axes=[0])\n",
        "    return (batch - mean) / tf.sqrt(var + tf.constant(1e-10))\n",
        "\n",
        "# Ajouter du bruit avec noise_std comme écart type\n",
        "def add_noise(inputs , noise_std):\n",
        "    return Lambda(lambda x: x + tf.random.normal(tf.shape(x)) * noise_std)(inputs)\n",
        "\n",
        "# Ladder Network\n",
        "def get_ladder_network_fc(layer_sizes=[784, 1000, 500, 250, 250, 250, 10], \n",
        "     noise_std=0.3,\n",
        "     \n",
        "     # Hyper-paramètres qui indiquent l'importance de chaque couche\n",
        "     denoising_cost=[1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]):\n",
        "    \n",
        "    # Nombre de couches\n",
        "    L = len(layer_sizes) - 1  \n",
        "\n",
        "    inputs_l = Input((layer_sizes[0],))  \n",
        "    inputs_u = Input((layer_sizes[0],))  \n",
        "\n",
        "    fc_enc = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[1:] ]\n",
        "    fc_dec = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[:-1]]\n",
        "    betas  = [AddBeta() for l in range(L)]\n",
        "\n",
        "    # Définir l'encodeur\n",
        "    def encoder(inputs, noise_std): \n",
        "        \n",
        "        # Ajouter du bruit\n",
        "        h = add_noise(inputs, noise_std)\n",
        "        all_z    = [None for _ in range( len(layer_sizes))]\n",
        "        all_z[0] = h\n",
        "        \n",
        "        for l in range(1, L+1):\n",
        "            z_pre = fc_enc[l-1](h)\n",
        "            z =     Lambda(batch_normalization)(z_pre) \n",
        "            z =     add_noise (z, noise_std)\n",
        "            \n",
        "            if l == L:\n",
        "                h = Activation('softmax')(betas[l-1](z))\n",
        "            else:\n",
        "                h = Activation('relu')(betas[l-1](z))\n",
        "                \n",
        "            all_z[l] = z\n",
        "\n",
        "        return h, all_z\n",
        "\n",
        "    y_c_l, _ = encoder(inputs_l, noise_std)\n",
        "    y_l, _   = encoder(inputs_l, 0.0) \n",
        "\n",
        "    y_c_u, corr_z  = encoder(inputs_u , noise_std)\n",
        "    y_u,  clean_z  = encoder(inputs_u , 0.0) \n",
        "\n",
        "    # Décodeur et débruitage\n",
        "    # Pour stocker la pré-activation, l'activation, la moyenne et la variance pour chaque couche\n",
        "    d_cost = [] \n",
        "    for l in range(L, -1, -1):\n",
        "        z, z_c = clean_z[l], corr_z[l]\n",
        "        if l == L:\n",
        "            u = y_c_u\n",
        "        else:\n",
        "            u = fc_dec[l]( z_est ) \n",
        "        u = Lambda(batch_normalization)(u)\n",
        "        z_est  = G_Guass()([z_c, u])  ######\n",
        "        d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est - z), 1)) / layer_sizes[l]) * denoising_cost[l])\n",
        "    \n",
        "    u_cost = tf.add_n(d_cost)\n",
        "\n",
        "    y_c_l = Lambda(lambda x: x[0])([y_c_l, y_l, y_c_u, y_u, u, z_est, z])\n",
        "    \n",
        "    tr_m = Model([inputs_l, inputs_u], y_c_l)\n",
        "    tr_m.add_loss(u_cost)\n",
        "    tr_m.compile(keras.optimizers.Adam(lr=0.02 ), 'categorical_crossentropy', metrics=['accuracy'])\n",
        "    tr_m.metrics_tensors = []\n",
        "    tr_m.metrics_names.append(\"den_loss\")\n",
        "    tr_m.metrics_tensors.append(u_cost)\n",
        "\n",
        "    te_m = Model(inputs_l, y_l)\n",
        "    tr_m.test_model = te_m\n",
        "\n",
        "    return tr_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXo6psWfsiXQ"
      },
      "source": [
        "# Une fonction qui permet de sélectionner [source:target] elements de chaque classe\n",
        "# Par exemple [0:10] renvoie les 10 premiers élements de chaque classe\n",
        "def select_samples(data, source, target, nbr_classes=4):\n",
        "    X, y = data\n",
        "    X_list, y_list = list(), list()\n",
        "    for i in range(nbr_classes):\n",
        "        X_with_class = X[y == i]        \n",
        "        ix = range(source, target)\n",
        "        [X_list.append(X_with_class[j]) for j in ix]\n",
        "        [y_list.append(i) for j in ix]\n",
        "\n",
        "    return np.asarray(X_list), np.asarray(y_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxeNMzRJskum"
      },
      "source": [
        "X_train_labeled, y_train_labeled = select_samples((X_train, y_train), 0, 10, nbr_classes=4)\n",
        "X_train_unlabeled, _ = select_samples((X_train, y_train), 10, 20, nbr_classes=4)\n",
        "\n",
        "# L'encodage one-hot \n",
        "# qui consiste à encoder une variable à K états sur K bits dont un seul prend la valeur 1\n",
        "y_train_labeled = to_categorical(y_train_labeled, 4)\n",
        "\n",
        "X_train_labeled_rep = np.concatenate([X_train_labeled]*(X_train_unlabeled.shape[0] // X_train_labeled.shape[0]))\n",
        "y_train_labeled_rep = np.concatenate([y_train_labeled]*(X_train_unlabeled.shape[0] // X_train_labeled.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7oPaqF90s0JJ",
        "outputId": "881e6912-2f0e-42cc-a3ff-464807fd0b66"
      },
      "source": [
        "#Pour différentes valeurs de l'écart-type du bruit\n",
        "#list_noise_std = [0.1,0.2,0.3,0.4,0.5] # pour comparer plusieurs sigma\n",
        "list_noise_std = [0.4]\n",
        "\n",
        "# Initialisation de la liste qui contiendra les performances (accuracy) pour chaque écart-type\n",
        "accuracy_list = list_noise_std.copy()\n",
        "\n",
        "num_epochs =  200\n",
        "    \n",
        "for i in range(1):\n",
        "    \n",
        "    # Initialisation du modèle \n",
        "    model = get_ladder_network_fc(layer_sizes=[16384, 1000, 500, 500, 500, 250, 4], noise_std=list_noise_std[i])\n",
        "\n",
        "    # Entrainement du modèle\n",
        "    history = model.fit([X_train_labeled_rep, X_train_unlabeled], y_train_labeled_rep, epochs=num_epochs)\n",
        "\n",
        "    #Affichage des courbes de perte et de performance\n",
        "    xc = range(num_epochs)\n",
        "    fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
        "    axs[0].plot(xc,history.history['loss'])\n",
        "    axs[0].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    fig.suptitle('Sigma = %f'% list_noise_std[i] ,fontsize=16)\n",
        "    axs[1].plot(xc,history.history['accuracy'])\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_title('Accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 7s 32ms/step - loss: 349.0822 - accuracy: 0.2125\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 301.9943 - accuracy: 0.4292\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 254.4891 - accuracy: 0.5208\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 200.9104 - accuracy: 0.6625\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 169.6442 - accuracy: 0.6292\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 143.7548 - accuracy: 0.6896\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 122.3640 - accuracy: 0.7271\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 106.6629 - accuracy: 0.7708\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 97.4159 - accuracy: 0.8583\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 91.7840 - accuracy: 0.7438\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 87.2347 - accuracy: 0.9125\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 84.4086 - accuracy: 0.8958\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 84.7713 - accuracy: 0.9292\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 84.1868 - accuracy: 0.8688\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 84.7185 - accuracy: 0.7979\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 83.5692 - accuracy: 0.8417\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 83.8292 - accuracy: 0.8646\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 81.8988 - accuracy: 0.9229\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 81.2448 - accuracy: 0.8958\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 79.4390 - accuracy: 0.9396\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 78.6985 - accuracy: 0.9062\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 76.6780 - accuracy: 0.9667\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 76.4833 - accuracy: 0.9729\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 74.5611 - accuracy: 0.9562\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 71.7309 - accuracy: 0.8958\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 71.2227 - accuracy: 0.9292\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 71.6165 - accuracy: 0.9125\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 69.4138 - accuracy: 0.9396\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 65.8121 - accuracy: 0.9396\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 65.0177 - accuracy: 0.9333\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 68.1571 - accuracy: 0.9458\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 64.0394 - accuracy: 0.9396\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 64.9872 - accuracy: 0.9729\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 60.5975 - accuracy: 0.9292\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 59.0234 - accuracy: 0.9833\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 60.1692 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 59.2694 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 56.1415 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 55.6846 - accuracy: 0.9729\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 55.4168 - accuracy: 0.9833\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 52.8524 - accuracy: 0.9396\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 51.0556 - accuracy: 0.9833\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 49.0399 - accuracy: 0.9833\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 51.6283 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 48.4082 - accuracy: 0.9562\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 47.8272 - accuracy: 0.9667\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 45.8225 - accuracy: 0.9396\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 44.1938 - accuracy: 0.9833\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 44.1392 - accuracy: 0.9125\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 41.1048 - accuracy: 0.9396\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 46.5328 - accuracy: 0.9396\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 42.6791 - accuracy: 0.9833\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 38.6882 - accuracy: 0.9729\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 37.7628 - accuracy: 0.9833\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 39.1507 - accuracy: 0.9500\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 36.7156 - accuracy: 0.9396\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 36.2426 - accuracy: 0.9833\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 33.3202 - accuracy: 0.9500\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 32.8769 - accuracy: 0.9667\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 33.1794 - accuracy: 0.9729\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 34.1801 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 33.1660 - accuracy: 0.9833\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 30.1645 - accuracy: 0.9833\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 29.3523 - accuracy: 0.9667\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 28.5025 - accuracy: 0.9667\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 27.8300 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 27.4605 - accuracy: 0.9833\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 27.5234 - accuracy: 0.9833\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 26.7052 - accuracy: 0.9833\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 24.5576 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 27.1381 - accuracy: 0.9833\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 27.3372 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 23.0518 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 23.7351 - accuracy: 0.9833\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 23.3549 - accuracy: 0.9833\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 25.1054 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 23.4996 - accuracy: 0.9833\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 21.9701 - accuracy: 0.9229\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 23.5369 - accuracy: 0.9667\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 22.9965 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 30.6145 - accuracy: 0.9562\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 22.0091 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 19.4922 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20.1802 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 26.3695 - accuracy: 0.9833\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 25.5920 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 22.4737 - accuracy: 0.9833\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 21.0855 - accuracy: 0.9833\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 21.8254 - accuracy: 0.9833\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 21.7917 - accuracy: 0.9500\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 25.1821 - accuracy: 0.9833\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 21.9082 - accuracy: 0.9333\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.2635 - accuracy: 0.9396\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 26.8929 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 24.3184 - accuracy: 0.9729\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 24.5297 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 21.8095 - accuracy: 0.9833\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 21.8288 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 22.3903 - accuracy: 0.9833\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20.3349 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 25.6952 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20.6996 - accuracy: 0.9562\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 23.3712 - accuracy: 0.9667\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 19.3144 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 19.5494 - accuracy: 0.9833\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 19.1396 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.5275 - accuracy: 0.9562\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 22.5937 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 21.0039 - accuracy: 0.9667\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.3695 - accuracy: 0.9833\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 19.0135 - accuracy: 0.9833\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 18.5592 - accuracy: 0.9833\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 19.0772 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 26.0894 - accuracy: 0.9667\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 23.1257 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 19.0574 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20.0642 - accuracy: 0.9833\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 18.6230 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.7744 - accuracy: 0.9833\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 19.6881 - accuracy: 0.9500\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 21.9405 - accuracy: 0.9833\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 19.7857 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 23.6471 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 21.7681 - accuracy: 0.9500\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 19.1806 - accuracy: 0.9833\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.7946 - accuracy: 0.9500\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 18.9207 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.5345 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 22.6689 - accuracy: 0.9667\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 19.3154 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.8811 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 18.8881 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 17.8826 - accuracy: 0.9667\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 19.7343 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 23.0575 - accuracy: 0.9833\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 18.4569 - accuracy: 0.9833\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 17.6453 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 19.4549 - accuracy: 0.9667\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 20.9046 - accuracy: 0.9396\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 18.9761 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 24.4667 - accuracy: 0.9833\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 21.1319 - accuracy: 0.9833\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.9066 - accuracy: 0.9833\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 20.0795 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 18.2663 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 18.0357 - accuracy: 0.9833\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 18.5040 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 19.9480 - accuracy: 0.9833\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 17.7520 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 18.5983 - accuracy: 0.9833\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 17.1109 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 18.5656 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 18.4044 - accuracy: 0.9833\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 17.2854 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 20.2779 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 15.8586 - accuracy: 0.9667\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 16.4926 - accuracy: 0.9833\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 17.8436 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 17.6938 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 17.2578 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.8323 - accuracy: 0.9729\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 16.5340 - accuracy: 0.9667\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 16.1212 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 17.0204 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 17.2006 - accuracy: 0.9833\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 17.5253 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 16.2422 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 17.7147 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 17.0713 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.3121 - accuracy: 0.9833\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 15.8489 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 19.8198 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 16.2066 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.5630 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 15.7659 - accuracy: 0.9833\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.8128 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 15.2254 - accuracy: 0.9667\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 17.2832 - accuracy: 0.9833\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 16.3406 - accuracy: 0.9333\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 17.3535 - accuracy: 0.9667\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.0235 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 14.9346 - accuracy: 0.9833\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.1931 - accuracy: 0.9833\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.4922 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15.9333 - accuracy: 0.9833\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 18.0956 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 15.1133 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 17.8960 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 21.4983 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 26.6128 - accuracy: 0.9500\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 26.5823 - accuracy: 0.9833\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 18.0990 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 24.6517 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 21.9273 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 20.9131 - accuracy: 0.9833\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 28.1384 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 21.7644 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 24.8759 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 28.0687 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 21.6381 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5bnQgrusnCm",
        "outputId": "34209edf-fd7d-4aca-c854-fa7d442e350a"
      },
      "source": [
        "y_test_pr = model.test_model.predict(X_test, batch_size=10)\n",
        "accuracy_list[i] = accuracy_score(y_test, y_test_pr.argmax(-1))\n",
        "print(f\"Test accuracy : {accuracy_list[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy : 0.425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3Ys4b810lEt",
        "outputId": "beadd48f-f72a-41c0-ca85-eb22ce6080b0"
      },
      "source": [
        "model.save('/content/drive/MyDrive/ladder network.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}